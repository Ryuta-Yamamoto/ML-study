{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import tanaka_corpus\n",
    "from prep import make_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = tanaka_corpus(is_train=True)\n",
    "test_sentences = tanaka_corpus(is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = make_data_set(*train_sentences, *test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz#egg=en_core_web_sm==2.3.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz (12.0MB)\n",
      "\u001b[K     |████████████████████████████████| 12.1MB 997kB/s eta 0:00:01    |█████████▍                      | 3.5MB 3.0MB/s eta 0:00:03     |██████████████████████          | 8.3MB 802kB/s eta 0:00:05     |████████████████████████████▍   | 10.7MB 821kB/s eta 0:00:02     |████████████████████████████▍   | 10.7MB 821kB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from en_core_web_sm==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: setuptools in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (41.2.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (0.7.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (4.47.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.19.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.24.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.25.9)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (3.1.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.0-cp37-none-any.whl size=12048610 sha256=cd4ae4a6111641fe03324aa7105c0a5d7c13da4ab3248fb5403f7f9809ff641a\n",
      "  Stored in directory: /private/var/folders/_0/f9sp8z895399jqc5ryb95d3m0000gn/T/pip-ephem-wheel-cache-szeqrqve/wheels/be/ec/e4/29a68c7de525fcfa2d898cc27d2b945fe7cd966feeb2d85e0b\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Collecting de_core_news_sm==2.3.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.3.0/de_core_news_sm-2.3.0.tar.gz#egg=de_core_news_sm==2.3.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.3.0/de_core_news_sm-2.3.0.tar.gz (14.9MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9MB 941kB/s eta 0:00:01    |█                               | 481kB 301kB/s eta 0:00:48     |█████▎                          | 2.5MB 687kB/s eta 0:00:19     |████████████████████▎           | 9.4MB 2.4MB/s eta 0:00:03     |████████████████████▌           | 9.5MB 2.4MB/s eta 0:00:03     |█████████████████████████▍      | 11.8MB 2.8MB/s eta 0:00:02     |██████████████████████████      | 12.1MB 2.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from de_core_news_sm==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.24.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (4.47.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.7.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.19.0)\n",
      "Requirement already satisfied: setuptools in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (41.2.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2020.6.20)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.1.0)\n",
      "Building wheels for collected packages: de-core-news-sm\n",
      "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.3.0-cp37-none-any.whl size=14907580 sha256=012c10ed4cf7155aae0a551ea2f691d6cdc5a919def5552c3a567005e8400e96\n",
      "  Stored in directory: /private/var/folders/_0/f9sp8z895399jqc5ryb95d3m0000gn/T/pip-ephem-wheel-cache-_ngm7tjk/wheels/db/f3/1e/0df0f27eee12bd1aaa94bcfef11b01eca62f90b9b9a0ce08fd\n",
      "Successfully built de-core-news-sm\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-2.3.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/de_core_news_sm\n",
      "-->\n",
      "/Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n"
     ]
    }
   ],
   "source": [
    "!poetry run python -m spacy download en\n",
    "!poetry run python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading training.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:03<00:00, 402kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading validation.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 93.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading mmt_task1_test2016.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 84.6kB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'zwei': 3873,\n",
       "         'junge': 2068,\n",
       "         'weiße': 146,\n",
       "         'männer': 1662,\n",
       "         'sind': 490,\n",
       "         'im': 3107,\n",
       "         'freien': 475,\n",
       "         'in': 11895,\n",
       "         'der': 4990,\n",
       "         'nähe': 383,\n",
       "         'vieler': 2,\n",
       "         'büsche': 5,\n",
       "         '.': 28809,\n",
       "         'mehrere': 580,\n",
       "         'mit': 8843,\n",
       "         'schutzhelmen': 33,\n",
       "         'bedienen': 9,\n",
       "         'ein': 18851,\n",
       "         'antriebsradsystem': 1,\n",
       "         'kleines': 772,\n",
       "         'mädchen': 2121,\n",
       "         'klettert': 171,\n",
       "         'spielhaus': 2,\n",
       "         'aus': 910,\n",
       "         'holz': 61,\n",
       "         'mann': 7805,\n",
       "         'einem': 13711,\n",
       "         'blauen': 992,\n",
       "         'hemd': 1202,\n",
       "         'steht': 1778,\n",
       "         'auf': 8745,\n",
       "         'einer': 6765,\n",
       "         'leiter': 56,\n",
       "         'und': 8925,\n",
       "         'putzt': 43,\n",
       "         'fenster': 148,\n",
       "         'stehen': 939,\n",
       "         'am': 911,\n",
       "         'herd': 17,\n",
       "         'bereiten': 79,\n",
       "         'essen': 377,\n",
       "         'zu': 1909,\n",
       "         'grün': 58,\n",
       "         'hält': 1310,\n",
       "         'eine': 9909,\n",
       "         'gitarre': 303,\n",
       "         ',': 8938,\n",
       "         'während': 1397,\n",
       "         'andere': 412,\n",
       "         'sein': 292,\n",
       "         'ansieht': 12,\n",
       "         'lächelt': 234,\n",
       "         'einen': 3479,\n",
       "         'ausgestopften': 3,\n",
       "         'löwen': 3,\n",
       "         'an': 3062,\n",
       "         'schickes': 4,\n",
       "         'spricht': 211,\n",
       "         'dem': 2134,\n",
       "         'handy': 129,\n",
       "         'sie': 429,\n",
       "         'langsam': 4,\n",
       "         'die': 3949,\n",
       "         'straße': 1412,\n",
       "         'entlangschwebt': 1,\n",
       "         'frau': 4186,\n",
       "         'großen': 565,\n",
       "         'geldbörse': 16,\n",
       "         'geht': 856,\n",
       "         'tor': 53,\n",
       "         'vorbei': 356,\n",
       "         'jungen': 536,\n",
       "         'tanzen': 139,\n",
       "         'mitten': 101,\n",
       "         'nacht': 58,\n",
       "         'pfosten': 20,\n",
       "         'ballettklasse': 2,\n",
       "         'fünf': 172,\n",
       "         'nacheinander': 4,\n",
       "         'springen': 139,\n",
       "         'vier': 396,\n",
       "         'typen': 19,\n",
       "         'von': 2363,\n",
       "         'denen': 91,\n",
       "         'drei': 1078,\n",
       "         'hüte': 12,\n",
       "         'tragen': 241,\n",
       "         'nicht': 64,\n",
       "         'oben': 128,\n",
       "         'treppenhaus': 9,\n",
       "         'schwarzer': 371,\n",
       "         'hund': 1606,\n",
       "         'gefleckter': 7,\n",
       "         'kämpfen': 87,\n",
       "         'neongrünen': 4,\n",
       "         'orangefarbenen': 223,\n",
       "         'uniform': 65,\n",
       "         'fährt': 835,\n",
       "         'grünen': 505,\n",
       "         'traktor': 16,\n",
       "         'frauen': 993,\n",
       "         'warten': 128,\n",
       "         'stadt': 259,\n",
       "         'schwarzem': 161,\n",
       "         'oberteil': 552,\n",
       "         'brille': 280,\n",
       "         'streut': 3,\n",
       "         'puderzucker': 2,\n",
       "         'gugelhupf': 1,\n",
       "         'sitzt': 1624,\n",
       "         'vor': 1936,\n",
       "         'gemalten': 4,\n",
       "         'regenbogen': 5,\n",
       "         'liegt': 254,\n",
       "         'bank': 244,\n",
       "         'auch': 28,\n",
       "         'weißer': 265,\n",
       "         'angebunden': 3,\n",
       "         'ist': 965,\n",
       "         'personen': 1152,\n",
       "         'sitzen': 925,\n",
       "         'instrumenten': 30,\n",
       "         'kreis': 36,\n",
       "         'gruppe': 1331,\n",
       "         'älterer': 299,\n",
       "         'spielt': 1307,\n",
       "         'zusammen': 292,\n",
       "         'klarinette': 5,\n",
       "         'notenblättern': 2,\n",
       "         'großes': 73,\n",
       "         'bauwerk': 7,\n",
       "         'kaputt': 1,\n",
       "         'gegangen': 2,\n",
       "         'fahrbahn': 2,\n",
       "         'große': 234,\n",
       "         'menschenmenge': 371,\n",
       "         'außen': 6,\n",
       "         'eingang': 20,\n",
       "         'metrostation': 1,\n",
       "         'tattoo': 8,\n",
       "         'seinem': 578,\n",
       "         'rücken': 105,\n",
       "         'erhält': 8,\n",
       "         'kinder': 792,\n",
       "         'kleinen': 376,\n",
       "         'wippe': 7,\n",
       "         'sand': 164,\n",
       "         'reflektierende': 1,\n",
       "         'weste': 85,\n",
       "         'schutzhelm': 49,\n",
       "         'trägt': 850,\n",
       "         'flagge': 65,\n",
       "         'person': 759,\n",
       "         'mantel': 176,\n",
       "         'belebten': 95,\n",
       "         'gehweg': 272,\n",
       "         'betrachtet': 132,\n",
       "         'gemälde': 21,\n",
       "         'straßenszene': 8,\n",
       "         'hosen': 188,\n",
       "         'läuft': 493,\n",
       "         'entlang': 699,\n",
       "         'das': 1261,\n",
       "         'kleine': 403,\n",
       "         'kind': 953,\n",
       "         'roten': 950,\n",
       "         'seilen': 15,\n",
       "         'spielplatz': 72,\n",
       "         'du': 6,\n",
       "         'weißt': 1,\n",
       "         'dass': 56,\n",
       "         'ich': 20,\n",
       "         'aussehe': 1,\n",
       "         'wie': 238,\n",
       "         'justin': 1,\n",
       "         'bieber': 1,\n",
       "         'junger': 455,\n",
       "         'schwarz-gelben': 3,\n",
       "         'jacke': 467,\n",
       "         'blickt': 266,\n",
       "         'etwas': 584,\n",
       "         'tasse': 35,\n",
       "         'kaffee': 31,\n",
       "         'urinal': 2,\n",
       "         'gehende': 14,\n",
       "         'mehrfarbigen': 7,\n",
       "         'himmel': 88,\n",
       "         'hintergrund': 479,\n",
       "         'alter': 131,\n",
       "         'allein': 26,\n",
       "         'bier': 67,\n",
       "         'trinkt': 101,\n",
       "         'geschulter': 1,\n",
       "         'polizeihund': 1,\n",
       "         'neben': 966,\n",
       "         'hundeführer': 1,\n",
       "         'polizeitransporter': 1,\n",
       "         'verschneiten': 64,\n",
       "         'fahrrad': 478,\n",
       "         'alle': 43,\n",
       "         'hemden': 86,\n",
       "         'krawatten': 4,\n",
       "         'schwarze': 150,\n",
       "         'freizeithosen': 1,\n",
       "         'unterhalten': 209,\n",
       "         'sich': 2273,\n",
       "         'hinter': 433,\n",
       "         'lieferwagen': 11,\n",
       "         'nach': 426,\n",
       "         'hinten': 68,\n",
       "         'gerichteten': 2,\n",
       "         'hut': 395,\n",
       "         'arbeitet': 250,\n",
       "         'maschinen': 9,\n",
       "         'arbeiten': 215,\n",
       "         'fabrikumgebung': 1,\n",
       "         'packen': 6,\n",
       "         'gläser': 3,\n",
       "         'kerzen': 16,\n",
       "         'kartons': 9,\n",
       "         'asiatischer': 83,\n",
       "         'kehrt': 23,\n",
       "         'den': 1575,\n",
       "         'lehnt': 123,\n",
       "         'auto': 175,\n",
       "         'um': 795,\n",
       "         'fahrer': 22,\n",
       "         'reden': 59,\n",
       "         'zusieht': 72,\n",
       "         'kleinkinder': 11,\n",
       "         'gras': 355,\n",
       "         'leute': 656,\n",
       "         'sehen': 272,\n",
       "         'seltsamen': 11,\n",
       "         'fahrzeug': 49,\n",
       "         'platz': 84,\n",
       "         'silbernen': 19,\n",
       "         'schöne': 24,\n",
       "         'braut': 50,\n",
       "         'ihrem': 229,\n",
       "         'neuen': 7,\n",
       "         'ehemann': 3,\n",
       "         'kleiner': 702,\n",
       "         'bei': 885,\n",
       "         \"mcdonald's\": 2,\n",
       "         'gamecube': 1,\n",
       "         'schüttelt': 30,\n",
       "         'rande': 14,\n",
       "         'eines': 1052,\n",
       "         'strands': 9,\n",
       "         'ball': 464,\n",
       "         'park': 257,\n",
       "         'grillen': 17,\n",
       "         'sonnenbrille': 195,\n",
       "         'legt': 55,\n",
       "         'seinen': 266,\n",
       "         'arm': 116,\n",
       "         'schwarz-weißen': 20,\n",
       "         'bluse': 141,\n",
       "         'luftballonhut': 1,\n",
       "         'picknicktischen': 3,\n",
       "         'taekwondo-wettbewerbs': 1,\n",
       "         'sprungtritt': 3,\n",
       "         'über': 1127,\n",
       "         'macht': 747,\n",
       "         'dabei': 266,\n",
       "         'tritt': 79,\n",
       "         'wasser': 818,\n",
       "         'weißen': 950,\n",
       "         'gießt': 26,\n",
       "         'sonne': 67,\n",
       "         'schützt': 9,\n",
       "         'versucht': 253,\n",
       "         'stück': 74,\n",
       "         'papier': 74,\n",
       "         'lesen': 51,\n",
       "         'kindern': 149,\n",
       "         'laufen': 253,\n",
       "         'overall': 32,\n",
       "         'steinwand': 4,\n",
       "         'springt': 731,\n",
       "         'baumstamm': 24,\n",
       "         'anzug': 144,\n",
       "         'rennt': 339,\n",
       "         'anderen': 461,\n",
       "         'herren': 11,\n",
       "         'herum': 276,\n",
       "         'barfuß': 31,\n",
       "         'olivgrüne': 1,\n",
       "         'kurze': 10,\n",
       "         'propangasgrill': 1,\n",
       "         'hotdogs': 6,\n",
       "         'grillt': 30,\n",
       "         'gleichzeitig': 14,\n",
       "         'blaue': 77,\n",
       "         'kunststofftasse': 2,\n",
       "         'schnee': 346,\n",
       "         'wartet': 147,\n",
       "         'bis': 33,\n",
       "         'ampel': 9,\n",
       "         'wird': 387,\n",
       "         'skiern': 32,\n",
       "         'verkaufende': 1,\n",
       "         'kunstwerke': 12,\n",
       "         'sieben': 33,\n",
       "         'kletterer': 20,\n",
       "         'klettern': 50,\n",
       "         'felswand': 40,\n",
       "         'hoch': 355,\n",
       "         'anderer': 149,\n",
       "         'dasteht': 2,\n",
       "         'seil': 82,\n",
       "         'gelenkige': 1,\n",
       "         'körper': 15,\n",
       "         'des': 263,\n",
       "         'turners': 1,\n",
       "         'schwebt': 11,\n",
       "         'schwebebalken': 7,\n",
       "         'schiebt': 113,\n",
       "         'spielzeug-geländefahrzeug': 1,\n",
       "         'gummi-pool': 1,\n",
       "         'windjacke': 4,\n",
       "         'dach': 68,\n",
       "         'installiertes': 1,\n",
       "         'fernrohr': 5,\n",
       "         'darunterliegende': 1,\n",
       "         'objekt': 30,\n",
       "         'flugzeug': 29,\n",
       "         'aussieht': 38,\n",
       "         'schlauch': 26,\n",
       "         'posieren': 141,\n",
       "         'glücklich': 23,\n",
       "         'einkaufswagen': 40,\n",
       "         'supermarkt': 7,\n",
       "         'kurz': 44,\n",
       "         'davor': 38,\n",
       "         'gelbes': 48,\n",
       "         'hundespielzeug': 7,\n",
       "         'fangen': 101,\n",
       "         'kerl': 27,\n",
       "         'dessen': 39,\n",
       "         'hand': 516,\n",
       "         'teil': 80,\n",
       "         'seines': 38,\n",
       "         'gesichts': 6,\n",
       "         'bedeckt': 42,\n",
       "         'nische': 2,\n",
       "         'restaurant': 151,\n",
       "         'schwarz-weißer': 58,\n",
       "         'spring': 10,\n",
       "         'gelben': 409,\n",
       "         'spielzeug': 108,\n",
       "         'wanderer': 63,\n",
       "         'machen': 235,\n",
       "         'stückchen': 3,\n",
       "         'pause': 45,\n",
       "         'führt': 170,\n",
       "         'seine': 277,\n",
       "         'neue': 14,\n",
       "         'hölzerne': 1,\n",
       "         'kreation': 1,\n",
       "         'vater': 52,\n",
       "         'erwachsener': 62,\n",
       "         'sohn': 48,\n",
       "         'camping-ausflug': 1,\n",
       "         'wildnis': 8,\n",
       "         'reisender': 1,\n",
       "         'bart': 87,\n",
       "         'karte': 13,\n",
       "         'liest': 210,\n",
       "         'winkt': 41,\n",
       "         'ente': 10,\n",
       "         'umgeben': 94,\n",
       "         'grünanlage': 1,\n",
       "         'paar': 356,\n",
       "         'baby': 240,\n",
       "         'sportwagen': 16,\n",
       "         'gebäude': 397,\n",
       "         'parkenden': 1,\n",
       "         'durch': 895,\n",
       "         'bohrt': 6,\n",
       "         'gefrorene': 3,\n",
       "         'eis': 49,\n",
       "         'teichs': 2,\n",
       "         'lohfarbene': 1,\n",
       "         'hunde': 352,\n",
       "         'spielen': 910,\n",
       "         'sandigen': 12,\n",
       "         'strand': 465,\n",
       "         'blau': 155,\n",
       "         'rot': 132,\n",
       "         'pickeln': 1,\n",
       "         'eisklettert': 1,\n",
       "         'pfad': 52,\n",
       "         'wiese': 149,\n",
       "         'gehen': 613,\n",
       "         'schwarz': 224,\n",
       "         'schaufelt': 21,\n",
       "         'ignoriert': 3,\n",
       "         'öffentliche': 4,\n",
       "         'sicherheit': 1,\n",
       "         'seiner': 190,\n",
       "         'hochzeitstorte': 6,\n",
       "         'nasser': 10,\n",
       "         'grünes': 42,\n",
       "         'dorfbewohner': 4,\n",
       "         'verkaufen': 42,\n",
       "         'ihre': 337,\n",
       "         'ernte': 3,\n",
       "         'markt': 96,\n",
       "         'vollen': 26,\n",
       "         'konzert': 43,\n",
       "         'nähert': 21,\n",
       "         'hauptsänger': 1,\n",
       "         'skateboard': 208,\n",
       "         'sieht': 213,\n",
       "         'befinden': 112,\n",
       "         'kajak': 38,\n",
       "         'ihnen': 181,\n",
       "         'baustelle': 46,\n",
       "         'arbeitenden': 2,\n",
       "         'männern': 150,\n",
       "         'reklamefläche': 1,\n",
       "         'werbung': 21,\n",
       "         'für': 480,\n",
       "         'brillen': 8,\n",
       "         'jugendlicher': 47,\n",
       "         'schwenkt': 17,\n",
       "         'fahnen': 30,\n",
       "         'farbspektrum': 1,\n",
       "         'zeigen': 38,\n",
       "         'alte': 82,\n",
       "         'er': 271,\n",
       "         'fisch': 51,\n",
       "         'zubereitet': 14,\n",
       "         'pullunder': 9,\n",
       "         'wallenden': 1,\n",
       "         'rock': 69,\n",
       "         'lied': 16,\n",
       "         'singend': 2,\n",
       "         'bühne': 286,\n",
       "         'treppenstufen': 12,\n",
       "         'brauner': 232,\n",
       "         'labrador': 5,\n",
       "         'wobei': 50,\n",
       "         'maul': 120,\n",
       "         'hat': 464,\n",
       "         'männlicher': 36,\n",
       "         'hockey-goalie': 2,\n",
       "         'roter': 147,\n",
       "         'duckt': 3,\n",
       "         'stock': 120,\n",
       "         'beim': 384,\n",
       "         'rucksack': 117,\n",
       "         'hof': 77,\n",
       "         'gebäudes': 89,\n",
       "         'skulptur': 17,\n",
       "         'männliches': 12,\n",
       "         'kleinkind': 117,\n",
       "         'geländer': 85,\n",
       "         'festhält': 15,\n",
       "         'kniet': 65,\n",
       "         'hochhaus': 1,\n",
       "         'ihr': 290,\n",
       "         'örtlichen': 11,\n",
       "         'spazieren': 94,\n",
       "         'obst': 50,\n",
       "         'blondhaariger': 7,\n",
       "         'dunkelhaariges': 5,\n",
       "         'kindertisch': 2,\n",
       "         'isst': 151,\n",
       "         'draußen': 292,\n",
       "         'tisch': 395,\n",
       "         'elektrogitarre': 4,\n",
       "         'softball': 14,\n",
       "         'schlägt': 90,\n",
       "         'fast': 23,\n",
       "         'senkrecht': 3,\n",
       "         'unten': 61,\n",
       "         'schwarzen': 702,\n",
       "         'schwarzes': 47,\n",
       "         'städtischen': 55,\n",
       "         'umgebung': 50,\n",
       "         'beton': 25,\n",
       "         'glättet': 3,\n",
       "         'hübschen': 9,\n",
       "         'decke': 77,\n",
       "         'einzelner': 19,\n",
       "         'abends': 13,\n",
       "         'brücke': 89,\n",
       "         'grüne': 45,\n",
       "         'bohnen': 2,\n",
       "         'grill': 50,\n",
       "         'felsvorsprung': 9,\n",
       "         'bergen': 47,\n",
       "         'rast': 21,\n",
       "         'viele': 197,\n",
       "         'menschen': 924,\n",
       "         'überqueren': 82,\n",
       "         'sehr': 161,\n",
       "         'hohe': 11,\n",
       "         'fußbrücke': 2,\n",
       "         'baumbewachsenen': 2,\n",
       "         'hügel': 131,\n",
       "         'ansammlung': 5,\n",
       "         'schlauchbooten': 3,\n",
       "         'blonde': 130,\n",
       "         'händchen': 11,\n",
       "         'halten': 205,\n",
       "         'grauem': 48,\n",
       "         'haar': 139,\n",
       "         'stuhl': 134,\n",
       "         'instrument': 48,\n",
       "         'bambus': 5,\n",
       "         'malt': 94,\n",
       "         'ziegel': 7,\n",
       "         'dreißigern': 1,\n",
       "         'u-bahn': 60,\n",
       "         'telefon': 36,\n",
       "         'fußgängerübergang': 8,\n",
       "         'bus': 91,\n",
       "         'leuten': 127,\n",
       "         'nudeln': 5,\n",
       "         'meeres': 13,\n",
       "         'schale': 6,\n",
       "         'voller': 137,\n",
       "         'ihrer': 118,\n",
       "         'typ': 69,\n",
       "         'loch': 37,\n",
       "         'lächeln': 116,\n",
       "         'viel': 25,\n",
       "         'groß': 4,\n",
       "         'männlich': 1,\n",
       "         'weiblich': 2,\n",
       "         'waldgebiet': 22,\n",
       "         'wanne': 7,\n",
       "         'schlafenden': 5,\n",
       "         'jemandem': 42,\n",
       "         'outfit': 65,\n",
       "         'rosafarbenen': 99,\n",
       "         'streifen': 13,\n",
       "         'mitglied': 9,\n",
       "         'afrikanischen': 10,\n",
       "         'stamms': 1,\n",
       "         'stammeskleidung': 7,\n",
       "         'konzentriert': 23,\n",
       "         'kamera': 363,\n",
       "         'samurai-krieger': 1,\n",
       "         'ganz': 43,\n",
       "         'übungsmatte': 1,\n",
       "         'nimmt': 93,\n",
       "         'schwert': 13,\n",
       "         'scheide': 1,\n",
       "         'überfüllten': 29,\n",
       "         'richtung': 87,\n",
       "         'weg': 202,\n",
       "         'steiniges': 2,\n",
       "         'flussbett': 1,\n",
       "         'wachmann': 8,\n",
       "         'beleuchteten': 33,\n",
       "         'metallskulptur': 2,\n",
       "         'violett': 9,\n",
       "         'gehsteig': 62,\n",
       "         'abwaschen': 1,\n",
       "         'ducken': 1,\n",
       "         'büschen': 15,\n",
       "         'sprechen': 50,\n",
       "         'telefone': 1,\n",
       "         'junges': 220,\n",
       "         'mehrfarbig': 1,\n",
       "         'gekleidet': 90,\n",
       "         'rechten': 23,\n",
       "         'haus': 83,\n",
       "         'leuchtend': 38,\n",
       "         'boden': 349,\n",
       "         'kunststück': 74,\n",
       "         'skateboarder': 36,\n",
       "         'lachen': 61,\n",
       "         'flasche': 35,\n",
       "         'streckt': 50,\n",
       "         'schütteln': 11,\n",
       "         'kommuniziert': 3,\n",
       "         'walkie-talkie': 3,\n",
       "         'fluss': 118,\n",
       "         'hinunterpaddelt': 1,\n",
       "         'gesehen': 10,\n",
       "         'zöpfen': 14,\n",
       "         'pullovern': 7,\n",
       "         'restauranttisch': 3,\n",
       "         'mahlzeit': 29,\n",
       "         'bauarbeiter': 117,\n",
       "         'stahlbalken': 2,\n",
       "         'strahlend': 4,\n",
       "         'teig': 17,\n",
       "         'schüssel': 32,\n",
       "         'rührt': 8,\n",
       "         'sonnigen': 89,\n",
       "         'tag': 188,\n",
       "         'luftballon': 10,\n",
       "         'matsch': 4,\n",
       "         'broschüre': 4,\n",
       "         'zugfahrten': 1,\n",
       "         'fußweg': 6,\n",
       "         'joggen': 20,\n",
       "         'sprungturm': 1,\n",
       "         ' ': 44,\n",
       "         'hohen': 84,\n",
       "         'brett': 18,\n",
       "         'schwimmbecken': 107,\n",
       "         'hell': 19,\n",
       "         'gefärbter': 2,\n",
       "         'räkelt': 1,\n",
       "         'hölzernen': 21,\n",
       "         'wasserbehälter': 1,\n",
       "         'schläft': 127,\n",
       "         'bushaltestelle': 21,\n",
       "         'jeans': 250,\n",
       "         'baum': 162,\n",
       "         'anderes': 48,\n",
       "         'schlafender': 5,\n",
       "         'fahrenden': 6,\n",
       "         'befindet': 169,\n",
       "         'unter': 329,\n",
       "         'wohnungsküche': 1,\n",
       "         'wirft': 161,\n",
       "         'pfannkuchen': 2,\n",
       "         'feuerwehrleute': 12,\n",
       "         'reagieren': 5,\n",
       "         'alarm': 1,\n",
       "         'löschfahrzeuge': 1,\n",
       "         'booten': 12,\n",
       "         'ufer': 55,\n",
       "         'schutzwesten': 5,\n",
       "         'zeitungsverkäufer': 1,\n",
       "         'farbenfrohen': 36,\n",
       "         'zusammenstellung': 1,\n",
       "         'zeitschriften': 8,\n",
       "         'blaues': 53,\n",
       "         'jeanslatzhose': 1,\n",
       "         'bäckerei': 4,\n",
       "         'dicke': 4,\n",
       "         'bäcker': 2,\n",
       "         'lehrling': 1,\n",
       "         'ihm': 222,\n",
       "         'mehl': 5,\n",
       "         'augen': 68,\n",
       "         'reibt': 3,\n",
       "         'nacktem': 55,\n",
       "         'oberkörper': 137,\n",
       "         'tropischen': 7,\n",
       "         'schatten': 43,\n",
       "         'badehosen': 7,\n",
       "         'skifahrer': 56,\n",
       "         'schneebedeckten': 91,\n",
       "         'berg': 126,\n",
       "         'rotes': 62,\n",
       "         'erdhügel': 10,\n",
       "         'zweiradfahrer': 1,\n",
       "         'rennen': 274,\n",
       "         'biegen': 1,\n",
       "         'scharf': 1,\n",
       "         'links': 31,\n",
       "         'ab': 125,\n",
       "         'fahren': 285,\n",
       "         'sandhügel': 7,\n",
       "         'mitte': 71,\n",
       "         'beugt': 41,\n",
       "         'mannes': 81,\n",
       "         'arzt': 10,\n",
       "         'pflegekräfte': 1,\n",
       "         'blauer': 161,\n",
       "         'arbeitskleidung': 24,\n",
       "         'führen': 63,\n",
       "         'operation': 11,\n",
       "         'flauschiger': 11,\n",
       "         'dobermann': 1,\n",
       "         'hinterher': 24,\n",
       "         'da': 75,\n",
       "         'nachdem': 27,\n",
       "         'bowlingkugel': 18,\n",
       "         'bahn': 26,\n",
       "         'geworfen': 21,\n",
       "         'wettergegerbter': 1,\n",
       "         'reitet': 123,\n",
       "         'schönen': 36,\n",
       "         'esel': 15,\n",
       "         'felsen': 173,\n",
       "         'couch': 31,\n",
       "         'schläfchen': 4,\n",
       "         'speer': 6,\n",
       "         'nachts': 70,\n",
       "         'ballons': 21,\n",
       "         'schulter': 40,\n",
       "         'zeitschrift': 17,\n",
       "         'hocke': 21,\n",
       "         'anzuzünden': 2,\n",
       "         'belebte': 39,\n",
       "         'einkaufszentrum': 18,\n",
       "         'verdunkelten': 5,\n",
       "         'raum': 149,\n",
       "         'hunden': 34,\n",
       "         'seifenschaum': 1,\n",
       "         'bedeckter': 6,\n",
       "         'bekommt': 30,\n",
       "         'gesicht': 180,\n",
       "         'abgewischt': 1,\n",
       "         'dutzende': 3,\n",
       "         'feiern': 23,\n",
       "         'boot': 192,\n",
       "         'hellen': 34,\n",
       "         'bikini': 41,\n",
       "         'cowboy-hut': 2,\n",
       "         'stroh': 5,\n",
       "         'rutscht': 59,\n",
       "         'rutsche': 53,\n",
       "         'farbigen': 14,\n",
       "         'rohren': 1,\n",
       "         'herunter': 86,\n",
       "         'taucheranzug': 6,\n",
       "         'krabbelkind': 1,\n",
       "         'luft': 466,\n",
       "         'bereit': 69,\n",
       "         'es': 157,\n",
       "         'aufzufangen': 1,\n",
       "         'lkw': 41,\n",
       "         'ländliche': 2,\n",
       "         'köche': 14,\n",
       "         'restaurantküche': 5,\n",
       "         'hamburger': 8,\n",
       "         'schneiden': 27,\n",
       "         'grimassen': 14,\n",
       "         'braunen': 257,\n",
       "         'eingezäunten': 15,\n",
       "         'bereich': 46,\n",
       "         'funken': 4,\n",
       "         'fliegen': 19,\n",
       "         'kreuz': 9,\n",
       "         'arbeitende': 5,\n",
       "         'schaufeln': 14,\n",
       "         'gleis': 6,\n",
       "         'pferde': 26,\n",
       "         'eingedämmten': 1,\n",
       "         'feuers': 2,\n",
       "         'arbeiter': 150,\n",
       "         'warnwesten': 6,\n",
       "         'zugwaggons': 2,\n",
       "         'klassischen': 2,\n",
       "         'schlichten': 1,\n",
       "         'volkswagen': 3,\n",
       "         'käfer': 3,\n",
       "         'baut': 21,\n",
       "         'metallrahmen': 2,\n",
       "         'starren': 14,\n",
       "         'dock': 11,\n",
       "         'maschine': 54,\n",
       "         'ausbessert': 1,\n",
       "         'rötlichem': 2,\n",
       "         'mascara': 3,\n",
       "         'wimpern': 1,\n",
       "         'kopf': 272,\n",
       "         'shorts': 170,\n",
       "         'halle': 8,\n",
       "         'motorroller': 15,\n",
       "         'geparkt': 20,\n",
       "         'polizeihubschrauber': 1,\n",
       "         'parks': 4,\n",
       "         'bereitet': 136,\n",
       "         'landung': 5,\n",
       "         'warnweste': 7,\n",
       "         'bringt': 25,\n",
       "         'gerüst': 35,\n",
       "         'position': 6,\n",
       "         'vierzehn': 1,\n",
       "         'bestehende': 17,\n",
       "         'esstischen': 2,\n",
       "         'versammelt': 101,\n",
       "         'teenager': 61,\n",
       "         'aufblasbaren': 18,\n",
       "         'familie': 101,\n",
       "         'ältere': 224,\n",
       "         'brät': 5,\n",
       "         'küche': 108,\n",
       "         'pfanne': 8,\n",
       "         'extravagante': 1,\n",
       "         'feder-kopfschmuck': 1,\n",
       "         'kommt': 53,\n",
       "         'waldland': 1,\n",
       "         'grünem': 85,\n",
       "         'offenen': 47,\n",
       "         'veranda': 29,\n",
       "         'handtasche': 29,\n",
       "         'tür': 54,\n",
       "         'formulare': 1,\n",
       "         'ausfüllt': 1,\n",
       "         'kreuzung': 19,\n",
       "         'lilafarben': 1,\n",
       "         'mieder': 1,\n",
       "         'faltet': 2,\n",
       "         'hände': 89,\n",
       "         'schoß': 46,\n",
       "         'mutter': 89,\n",
       "         'fischen': 21,\n",
       "         'strandpromenade': 6,\n",
       "         'sechs': 76,\n",
       "         'männliche': 59,\n",
       "         'spiel': 105,\n",
       "         'paddeln': 16,\n",
       "         'lacht': 76,\n",
       "         'motor': 7,\n",
       "         'alten': 74,\n",
       "         'antiken': 4,\n",
       "         'autos': 91,\n",
       "         'gelber': 68,\n",
       "         'benzinkanister': 1,\n",
       "         'wassernähe': 3,\n",
       "         'braunes': 22,\n",
       "         'pony': 6,\n",
       "         'ziehen': 41,\n",
       "         'gelenkten': 1,\n",
       "         'wagen': 89,\n",
       "         'untergrund': 7,\n",
       "         'steile': 9,\n",
       "         'verwendet': 32,\n",
       "         'dazu': 15,\n",
       "         'wandergruppe': 1,\n",
       "         'kalkstein': 1,\n",
       "         'erwachsenen': 54,\n",
       "         'so': 32,\n",
       "         'tut': 12,\n",
       "         'als': 245,\n",
       "         'ob': 25,\n",
       "         'ins': 145,\n",
       "         'gerade': 96,\n",
       "         'satz': 3,\n",
       "         'unterricht': 5,\n",
       "         'wort': 3,\n",
       "         'melden': 1,\n",
       "         'badehose': 39,\n",
       "         'betonplattform': 1,\n",
       "         'gewässer': 92,\n",
       "         'nähmaschine': 19,\n",
       "         'präzise': 1,\n",
       "         'dame': 238,\n",
       "         't-shirt': 354,\n",
       "         'ihren': 321,\n",
       "         'farm': 2,\n",
       "         'paprika': 1,\n",
       "         'verkauft': 103,\n",
       "         'rasen': 72,\n",
       "         'blasen': 12,\n",
       "         'geblasen': 1,\n",
       "         'hindernisstrecke': 1,\n",
       "         'tunnel': 24,\n",
       "         'heraus': 28,\n",
       "         'erdboden': 5,\n",
       "         'badeanzug': 49,\n",
       "         'blumenaufdruck': 1,\n",
       "         'meer': 155,\n",
       "         'reihe': 93,\n",
       "         'farbenfroher': 16,\n",
       "         'pool': 69,\n",
       "         'schwimmbad': 17,\n",
       "         'diskutieren': 15,\n",
       "         'schneemobil-tour': 1,\n",
       "         'daran': 9,\n",
       "         'entfernen': 9,\n",
       "         'klappstuhl': 10,\n",
       "         'stapel': 22,\n",
       "         'dämmerung': 10,\n",
       "         'städtische': 18,\n",
       "         'haaren': 204,\n",
       "         'bläst': 30,\n",
       "         'blütenblätter': 2,\n",
       "         'blume': 18,\n",
       "         'geländewagen': 11,\n",
       "         'allradfahrzeug': 2,\n",
       "         'fliegt': 88,\n",
       "         'dunklen': 86,\n",
       "         'geben': 24,\n",
       "         'spachtel': 2,\n",
       "         'essbares': 1,\n",
       "         'eisenpfanne': 1,\n",
       "         'umarmen': 33,\n",
       "         'hinterteil': 4,\n",
       "         'kuh': 24,\n",
       "         'schiff': 12,\n",
       "         'wo': 40,\n",
       "         'ankunft': 2,\n",
       "         'erwarten': 1,\n",
       "         'sandalen': 42,\n",
       "         'uns': 7,\n",
       "         'singt': 182,\n",
       "         'mikrophon': 46,\n",
       "         'reiten': 40,\n",
       "         'elefanten': 13,\n",
       "         'hausähnlichen': 1,\n",
       "         'gebäuden': 47,\n",
       "         'bäumen': 119,\n",
       "         'mexikanischer': 1,\n",
       "         'motorhaube': 10,\n",
       "         'lkws': 4,\n",
       "         'rothaariges': 14,\n",
       "         'menschengruppe': 20,\n",
       "         'sammelt': 15,\n",
       "         'verschieden': 8,\n",
       "         'weißes': 83,\n",
       "         'weint': 21,\n",
       "         'wegen': 6,\n",
       "         'umgekippten': 3,\n",
       "         'spielzeugs': 2,\n",
       "         'fußballplatz': 4,\n",
       "         'fußball': 173,\n",
       "         'gefriergerät': 1,\n",
       "         'absticht': 1,\n",
       "         'hockern': 2,\n",
       "         'ausgebreitete': 1,\n",
       "         'amerikanische': 26,\n",
       "         'fahne': 16,\n",
       "         'falsche': 2,\n",
       "         'zeigt': 214,\n",
       "         'bild': 177,\n",
       "         'radfahrer': 111,\n",
       "         'kurvigen': 1,\n",
       "         'hinauf': 80,\n",
       "         'jeansjacke': 16,\n",
       "         'feld': 288,\n",
       "         'blauem': 180,\n",
       "         'daumen': 21,\n",
       "         'küssendes': 1,\n",
       "         'aufzug': 8,\n",
       "         'fußballspieler': 87,\n",
       "         'fußballdress': 11,\n",
       "         'händen': 122,\n",
       "         'teamkollegen': 3,\n",
       "         'aufgehalten': 1,\n",
       "         'spieler': 133,\n",
       "         'gegnerischen': 34,\n",
       "         'mannschaft': 53,\n",
       "         'greift': 59,\n",
       "         'bauen': 28,\n",
       "         'wand': 204,\n",
       "         'wüste': 30,\n",
       "         'freunden': 34,\n",
       "         'gesichtsfarbe': 2,\n",
       "         'federstirnbändern': 1,\n",
       "         'umhängetasche': 4,\n",
       "         'jackett': 12,\n",
       "         'seilspielzeug': 3,\n",
       "         'altmodische': 3,\n",
       "         'videokamera': 6,\n",
       "         'komischer': 2,\n",
       "         'kleidung': 450,\n",
       "         'flippigen': 1,\n",
       "         'teams': 26,\n",
       "         'verhindern': 4,\n",
       "         'tagsüber': 9,\n",
       "         'schürze': 64,\n",
       "         'bordsteinkante': 6,\n",
       "         'braun-weißer': 33,\n",
       "         'frisbee-scheibe': 10,\n",
       "         'zuschauer': 83,\n",
       "         'zusehen': 69,\n",
       "         'tennisspieler': 28,\n",
       "         'tennisanlage': 1,\n",
       "         'jongleure': 1,\n",
       "         'fackeln': 5,\n",
       "         'vorführung': 8,\n",
       "         'dichte': 1,\n",
       "         'stufen': 64,\n",
       "         'ohne': 50,\n",
       "         'schuhe': 38,\n",
       "         'glatten': 2,\n",
       "         'steinen': 27,\n",
       "         'gap-hut': 1,\n",
       "         'dummes': 2,\n",
       "         'kindes': 13,\n",
       "         'aufhängen': 1,\n",
       "         'speedo-oberteil': 1,\n",
       "         'schutzbrille': 19,\n",
       "         'atem': 3,\n",
       "         'vielbefahrene': 1,\n",
       "         'damen': 55,\n",
       "         'kartenspiel': 6,\n",
       "         'trinken': 70,\n",
       "         'genießen': 52,\n",
       "         'zusammenkunft': 3,\n",
       "         'pabst': 2,\n",
       "         'blue': 4,\n",
       "         'ribbon': 2,\n",
       "         'diet': 1,\n",
       "         'coke': 1,\n",
       "         'schuhen': 49,\n",
       "         'weißem': 184,\n",
       "         'grauen': 183,\n",
       "         'froschskulptur': 1,\n",
       "         'zeitung': 82,\n",
       "         'rote': 92,\n",
       "         'vogel': 43,\n",
       "         'sonnenblumenkerne': 1,\n",
       "         'tennisball': 61,\n",
       "         ...})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "def gen_batch()\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_vcab = 1000\n",
    "# trg_vcab = 1000\n",
    "\n",
    "class MyTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        src_emb: nn.Module, \n",
    "        trg_emb: nn.Module,\n",
    "        transformer: nn.Module,\n",
    "        device: Optional[torch.device]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.src_emb = src_emb\n",
    "        self.trg_emb = trg_emb\n",
    "        self.transformer = transformer\n",
    "        self.out = nn.Linear(\n",
    "            in_features=transformer.d_model,\n",
    "            out_features=trg_emb.num_embeddings\n",
    "        )\n",
    "        if not device:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, src: Tensor, trg: Tensor):\n",
    "        src_tensor = self.src_emb(src)\n",
    "        trg_tensor = self.trg_emb(trg)\n",
    "        hidden = self.transformer(src_tensor, trg_tensor)\n",
    "        return self.out(hidden)\n",
    "        \n",
    "\n",
    "def make_transformer(\n",
    "        src_vcab: int,\n",
    "        trg_vcab: int,\n",
    "        embedding_dim: int,\n",
    "        n_head: int = 16,\n",
    "        num_encoder_layers: int = 3\n",
    "):\n",
    "    src_emb = nn.Embedding(\n",
    "        num_embeddings=src_vcab, \n",
    "        embedding_dim=embedding_dim\n",
    "    )\n",
    "    trg_emb = nn.Embedding(\n",
    "        num_embeddings=trg_vcab,\n",
    "        embedding_dim=embedding_dim\n",
    "    )\n",
    "    transformer = nn.Transformer(\n",
    "        d_model=embedding_dim,\n",
    "        nhead=n_head, \n",
    "        num_encoder_layers=num_encoder_layers\n",
    "    )\n",
    "    return MyTransformer(\n",
    "        src_emb=src_emb,\n",
    "        trg_emb=trg_emb,\n",
    "        transformer=transformer\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_transformer(\n",
    "    src_vcab=len(SRC.vocab.itos),\n",
    "    trg_vcab=len(TRG.vocab.itos),\n",
    "    embedding_dim=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-c8840fc2f390>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ce6ab984d257>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-32ade810506a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msrc_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtrg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the feature number of src and tgt must be equal to d_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n\u001b[1;32m    122\u001b[0m                               \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0msrc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    934\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[1;32m    935\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = next(iter(train_iterator))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.src.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 128])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iterator)).trg.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: BucketIterator,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, batch in enumerate(iterator):\n",
    "\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-39460643b76c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self,\n\u001b[1;32m      3\u001b[0m                  \u001b[0menc_hid_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0mdec_hid_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  attn_dim: int):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
    "\n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tensor:\n",
    "\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((\n",
    "            repeated_decoder_hidden,\n",
    "            encoder_outputs),\n",
    "            dim = 2)))\n",
    "\n",
    "        attention = torch.sum(energy, dim=2)\n",
    "\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: int,\n",
    "                 attention: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "\n",
    "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def _weighted_encoder_rep(self,\n",
    "                              decoder_hidden: Tensor,\n",
    "                              encoder_outputs: Tensor) -> Tensor:\n",
    "\n",
    "        a = self.attention(decoder_hidden, encoder_outputs)\n",
    "\n",
    "        a = a.unsqueeze(1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
    "\n",
    "        return weighted_encoder_rep\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                input: Tensor,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
    "                                                          encoder_outputs)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
    "\n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
    "\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
    "\n",
    "        output = self.out(torch.cat((output,\n",
    "                                     weighted_encoder_rep,\n",
    "                                     embedded), dim = 1))\n",
    "\n",
    "        return output, decoder_hidden.squeeze(0)\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module,\n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
    "\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> token\n",
    "        output = trg[0,:]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "# ENC_EMB_DIM = 256\n",
    "# DEC_EMB_DIM = 256\n",
    "# ENC_HID_DIM = 512\n",
    "# DEC_HID_DIM = 512\n",
    "# ATTN_DIM = 64\n",
    "# ENC_DROPOUT = 0.5\n",
    "# DEC_DROPOUT = 0.5\n",
    "\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = 64\n",
    "DEC_HID_DIM = 64\n",
    "ATTN_DIM = 8\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "\n",
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor(data_set.train_origin.seqs.T)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, src, trg):\n",
    "        self.src = Tensor(src)\n",
    "        self.trg = Tensor(trg)\n",
    "        \n",
    "    def __getitem__(self, n):\n",
    "        return self.src[n], self.trg[n]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextDataset(data_set.train_origin.seqs.T, data_set.train_trans.seqs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module,\n",
    "          loader: DataLoader,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, (src, trg) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-76691126a836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-4a356c60b297>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-32ade810506a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msrc_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtrg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-study",
   "language": "python",
   "name": "ml-study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
