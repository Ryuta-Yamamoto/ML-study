{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import tanaka_corpus\n",
    "from prep import make_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = tanaka_corpus(is_train=True)\n",
    "test_sentences = tanaka_corpus(is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = make_data_set(*train_sentences, *test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz#egg=en_core_web_sm==2.3.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from en_core_web_sm==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.19.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: setuptools in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (41.2.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (0.7.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.24.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (4.47.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2020.6.20)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Requirement already satisfied: de_core_news_sm==2.3.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.3.0/de_core_news_sm-2.3.0.tar.gz#egg=de_core_news_sm==2.3.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from de_core_news_sm==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (41.2.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.19.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (4.47.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.25.9)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/de_core_news_sm\n",
      "-->\n",
      "/Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n",
      "Collecting ja_core_news_md==2.3.1 from https://github.com/explosion/spacy-models/releases/download/ja_core_news_md-2.3.1/ja_core_news_md-2.3.1.tar.gz#egg=ja_core_news_md==2.3.1\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/ja_core_news_md-2.3.1/ja_core_news_md-2.3.1.tar.gz (38.8MB)\n",
      "\u001b[K     |████████████████████████████████| 38.8MB 582kB/s eta 0:00:0105     |███████▏                        | 8.6MB 866kB/s eta 0:00:35     |████████▌                       | 10.3MB 845kB/s eta 0:00:34     |███████████████▉                | 19.3MB 1.8MB/s eta 0:00:12�███████████████                | 19.5MB 1.8MB/s eta 0:00:12�███████████████▏               | 19.6MB 1.8MB/s eta 0:00:11     |████████████████████▉           | 25.2MB 610kB/s eta 0:00:23 eta 0:00:32kB/s eta 0:00:32\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from ja_core_news_md==2.3.1) (2.3.0)\n",
      "Collecting sudachipy>=0.4.5 (from ja_core_news_md==2.3.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/bf/2e5e4ed553aecfcff85e492c96ae39d8413b54a2b356dea9905fa66cf31d/SudachiPy-0.4.9-cp37-cp37m-macosx_10_14_x86_64.whl (295kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 3.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sudachidict_core>=20200330 (from ja_core_news_md==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/e6/022619602aa53d76378d61a5136a5d3cf43964c12715085d7e7c250f4d54/SudachiDict-core-20200330.tar.gz\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (4.47.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (1.19.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (0.7.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: setuptools in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (41.2.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (2.24.0)\n",
      "Collecting dartsclone~=0.9.0 (from sudachipy>=0.4.5->ja_core_news_md==2.3.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/8a/4409d6476234403ad8eea9cee23f3da81e1d52555385651f07f1db166f24/dartsclone-0.9.0-cp37-cp37m-macosx_10_13_x86_64.whl (113kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 436kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sortedcontainers~=2.1.0 (from sudachipy>=0.4.5->ja_core_news_md==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/13/f3/cf85f7c3a2dbd1a515d51e1f1676d971abe41bba6f4ab5443240d9a78e5b/sortedcontainers-2.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (1.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (2020.6.20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Cython (from dartsclone~=0.9.0->sudachipy>=0.4.5->ja_core_news_md==2.3.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/4d/899fb0967be8b02bdbc47f892b48f576f9c2d28f7d49710bcecf9ed1c705/Cython-0.29.21-cp37-cp37m-macosx_10_9_x86_64.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 366kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /Users/ryutayamamoto/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->ja_core_news_md==2.3.1) (3.1.0)\n",
      "Building wheels for collected packages: ja-core-news-md, sudachidict-core\n",
      "  Building wheel for ja-core-news-md (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ja-core-news-md: filename=ja_core_news_md-2.3.1-cp37-none-any.whl size=39028868 sha256=8a638a19a53eb0b5d62f3962690f8a56cfd239a1d1be67fea8c3cf1a4a79e89d\n",
      "  Stored in directory: /private/var/folders/_0/f9sp8z895399jqc5ryb95d3m0000gn/T/pip-ephem-wheel-cache-_o4qxp2u/wheels/5e/3c/e2/f15e974be2d7ab2420d666571207fa8e18cebc1ce7de3201cb\n",
      "  Building wheel for sudachidict-core (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sudachidict-core: filename=SudachiDict_core-20200330-cp37-none-any.whl size=71080894 sha256=96dc111dc5e57c8679f1704e7afed01928acdd577b960e22aa83ebf7e2223621\n",
      "  Stored in directory: /private/var/folders/_0/f9sp8z895399jqc5ryb95d3m0000gn/T/pip-ephem-wheel-cache-_o4qxp2u/wheels/fc/69/08/4cbe227806f9a433907d311020e1cb303484e4a0f3b56471b7\n",
      "Successfully built ja-core-news-md sudachidict-core\n",
      "Installing collected packages: Cython, dartsclone, sortedcontainers, sudachipy, sudachidict-core, ja-core-news-md\n",
      "Successfully installed Cython-0.29.21 dartsclone-0.9.0 ja-core-news-md-2.3.1 sortedcontainers-2.1.0 sudachidict-core-20200330 sudachipy-0.4.9\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('ja_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!poetry run python -m spacy download en\n",
    "!poetry run python -m spacy download de\n",
    "!poetry run python -m spacy download ja_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k, TranslationDataset\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"ja_core_news_md\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "train_data = TranslationDataset(\n",
    "    path=\"data/tanaka_corpus/small_parallel_enja/dev.\",\n",
    "    exts=(\"en\", \"ja\"),\n",
    "    fields = (SRC, TRG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator,  = BucketIterator.splits(\n",
    "    (train_data,),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from typing import Tuple, Optional\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_vcab = 1000\n",
    "# trg_vcab = 1000\n",
    "\n",
    "class MyTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        src_emb: nn.Module, \n",
    "        trg_emb: nn.Module,\n",
    "        transformer: nn.Module,\n",
    "        pad_index: int,\n",
    "        device: Optional[torch.device] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.src_emb = src_emb\n",
    "        self.trg_emb = trg_emb\n",
    "        self.transformer = transformer\n",
    "        self.pad_index = pad_index\n",
    "        self.out = nn.Linear(\n",
    "            in_features=transformer.d_model,\n",
    "            out_features=trg_emb.num_embeddings\n",
    "        )\n",
    "        if not device:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, src: Tensor, trg: Tensor):\n",
    "        src_tensor = self.src_emb(src)\n",
    "        trg_tensor = self.trg_emb(trg)\n",
    "        trg_mask = torch.triu(torch.ones(len(trg_tensor), len(trg_tensor))).T\n",
    "        pad_mask = (src == self.pad_index).T\n",
    "        hidden = self.transformer(\n",
    "            src_tensor, \n",
    "            trg_tensor, \n",
    "            tgt_mask=trg_mask, \n",
    "            src_key_padding_mask=pad_mask\n",
    "        )\n",
    "        return self.out(hidden)\n",
    "        \n",
    "\n",
    "def make_transformer(\n",
    "        src_vcab: int,\n",
    "        trg_vcab: int,\n",
    "        embedding_dim: int,\n",
    "        n_head: int = 16,\n",
    "        num_encoder_layers: int = 3\n",
    "):\n",
    "    src_emb = nn.Embedding(\n",
    "        num_embeddings=src_vcab, \n",
    "        embedding_dim=embedding_dim\n",
    "    )\n",
    "    trg_emb = nn.Embedding(\n",
    "        num_embeddings=trg_vcab,\n",
    "        embedding_dim=embedding_dim\n",
    "    )\n",
    "    transformer = nn.Transformer(\n",
    "        d_model=embedding_dim,\n",
    "        nhead=n_head, \n",
    "        num_encoder_layers=num_encoder_layers\n",
    "    )\n",
    "    return MyTransformer(\n",
    "        src_emb=src_emb,\n",
    "        trg_emb=trg_emb,\n",
    "        transformer=transformer,\n",
    "        pad_index=SRC.vocab.stoi[SRC.pad_token]\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_transformer(\n",
    "    src_vcab=len(SRC.vocab.itos),\n",
    "    trg_vcab=len(TRG.vocab.itos),\n",
    "    embedding_dim=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: BucketIterator,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, batch in enumerate(iterator):\n",
    "\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(\n",
    "        start_time: int,\n",
    "        end_time: int\n",
    "):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(\n",
    "        model: nn.Module,\n",
    "        iterator: BucketIterator\n",
    "):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            \n",
    "            sos = torch.tensor([[TRG.vocab.stoi[TRG.init_token]] * src.shape[1]])\n",
    "            trg = sos\n",
    "            \n",
    "            while not is_end(trg):\n",
    "                output = model(src, trg)\n",
    "#                 trg = torch.cat([trg, choice(output)])\n",
    "                arr = output.argmax(axis=-1).reshape((output.shape[0], -1))\n",
    "                trg = torch.cat([sos, arr])\n",
    "    return trg\n",
    "\n",
    "\n",
    "def choice(tensor):\n",
    "    return tensor[-1].argmax(axis=-1).reshape((1, -1))\n",
    "\n",
    "def is_end(tensor):\n",
    "    # 20単語以上で終了\n",
    "    return tensor.shape[0] >= 20\n",
    "\n",
    "def to_text(seqs):\n",
    "    return [' '.join(arr) for arr in np.array(TRG.vocab.itos)[seqs].T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 20s\n",
      "\tTrain Loss: 5.206 | Train PPL: 182.290\n",
      "Epoch: 02 | Time: 0m 15s\n",
      "\tTrain Loss: 4.466 | Train PPL:  87.019\n",
      "Epoch: 03 | Time: 0m 24s\n",
      "\tTrain Loss: 4.331 | Train PPL:  76.039\n",
      "Epoch: 04 | Time: 0m 18s\n",
      "\tTrain Loss: 4.230 | Train PPL:  68.703\n",
      "Epoch: 05 | Time: 0m 17s\n",
      "\tTrain Loss: 4.300 | Train PPL:  73.668\n",
      "Epoch: 06 | Time: 0m 22s\n",
      "\tTrain Loss: 3.712 | Train PPL:  40.937\n",
      "Epoch: 07 | Time: 0m 21s\n",
      "\tTrain Loss: 3.344 | Train PPL:  28.337\n",
      "Epoch: 08 | Time: 0m 26s\n",
      "\tTrain Loss: 2.917 | Train PPL:  18.479\n",
      "Epoch: 09 | Time: 0m 20s\n",
      "\tTrain Loss: 2.379 | Train PPL:  10.791\n",
      "Epoch: 10 | Time: 0m 22s\n",
      "\tTrain Loss: 1.953 | Train PPL:   7.049\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "#     valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "\n",
    "# test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "# print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 128, 380])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([o, o]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-5d71a3569367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTRG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTRG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'src' is not defined"
     ]
    }
   ],
   "source": [
    "trg = torch.tensor([[TRG.vocab.stoi[TRG.init_token]] * len(src)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 380])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0, 0].reshape((1, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones((2, 2))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8, 8, 8,  ..., 7, 8, 7],\n",
       "        [3, 0, 3,  ..., 7, 0, 3],\n",
       "        [0, 0, 0,  ..., 0, 0, 3],\n",
       "        ...,\n",
       "        [8, 3, 3,  ..., 3, 3, 0],\n",
       "        [0, 3, 3,  ..., 3, 3, 3],\n",
       "        [3, 3, 3,  ..., 3, 3, 3]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1723, -0.9570, -1.3386,  ..., -0.0545, -0.2099, -0.4432],\n",
       "        [ 0.4067, -1.1410, -1.3601,  ...,  0.2576, -0.5417, -0.4260],\n",
       "        [-1.9829, -2.1971, -1.9372,  ..., -1.6308, -0.5584, -1.1724],\n",
       "        ...,\n",
       "        [-1.9829, -2.1971, -1.9372,  ..., -1.6308, -0.5584, -1.1724],\n",
       "        [-1.9829, -2.1971, -1.9372,  ..., -1.6308, -0.5584, -1.1724],\n",
       "        [-1.9829, -2.1971, -1.9372,  ..., -1.6308, -0.5584, -1.1724]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[:, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1728, -0.9571, -1.3386,  ..., -0.0545, -0.2097, -0.4432],\n",
       "        [ 0.4062, -1.1411, -1.3600,  ...,  0.2576, -0.5414, -0.4259],\n",
       "        [-1.9829, -2.1972, -1.9371,  ..., -1.6308, -0.5582, -1.1725],\n",
       "        ...,\n",
       "        [-1.9829, -2.1972, -1.9371,  ..., -1.6308, -0.5582, -1.1725],\n",
       "        [-1.9829, -2.1972, -1.9371,  ..., -1.6308, -0.5582, -1.1725],\n",
       "        [-1.9829, -2.1972, -1.9371,  ..., -1.6308, -0.5582, -1.1725]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, o = estimate(model, train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  2,  2,  ...,  2,  2,  2],\n",
       "        [13, 13, 13,  ..., 13, 13, 13],\n",
       "        [13, 13, 13,  ..., 13, 13, 13],\n",
       "        ...,\n",
       "        [ 5,  5,  5,  ...,  5,  5,  5],\n",
       "        [ 7,  7,  7,  ...,  7,  7,  7],\n",
       "        [ 7,  7,  7,  ...,  7,  7,  7]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = estimate(model, train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2,   2,   2,  ...,   2,   2,   2],\n",
       "        [  0, 174,  15,  ..., 306,  17,  15],\n",
       "        [172,  50,   5,  ...,   7,   9,   5],\n",
       "        ...,\n",
       "        [  1,   3,   1,  ...,   1,   1,   1],\n",
       "        [  1,   1,   1,  ...,   1,   1,   1],\n",
       "        [  1,   1,   1,  ...,   1,   1,   1]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iterator.__iter__()).trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape[0] == 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1594, -2.1974, -2.0302,  ..., -1.4385, -1.2097, -1.6356],\n",
       "         [-1.1594, -2.1973, -2.0301,  ..., -1.4385, -1.2096, -1.6357],\n",
       "         [-1.1595, -2.1972, -2.0302,  ..., -1.4383, -1.2096, -1.6357],\n",
       "         ...,\n",
       "         [-1.1601, -2.1969, -2.0299,  ..., -1.4383, -1.2094, -1.6356],\n",
       "         [-1.1593, -2.1974, -2.0301,  ..., -1.4384, -1.2096, -1.6357],\n",
       "         [-1.1592, -2.1974, -2.0302,  ..., -1.4384, -1.2095, -1.6357]]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = next(iter(train_iterator))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.src.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 128])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iterator)).trg.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-39460643b76c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self,\n\u001b[1;32m      3\u001b[0m                  \u001b[0menc_hid_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0mdec_hid_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  attn_dim: int):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
    "\n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tensor:\n",
    "\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((\n",
    "            repeated_decoder_hidden,\n",
    "            encoder_outputs),\n",
    "            dim = 2)))\n",
    "\n",
    "        attention = torch.sum(energy, dim=2)\n",
    "\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: int,\n",
    "                 attention: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "\n",
    "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def _weighted_encoder_rep(self,\n",
    "                              decoder_hidden: Tensor,\n",
    "                              encoder_outputs: Tensor) -> Tensor:\n",
    "\n",
    "        a = self.attention(decoder_hidden, encoder_outputs)\n",
    "\n",
    "        a = a.unsqueeze(1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
    "\n",
    "        return weighted_encoder_rep\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                input: Tensor,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
    "                                                          encoder_outputs)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
    "\n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
    "\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
    "\n",
    "        output = self.out(torch.cat((output,\n",
    "                                     weighted_encoder_rep,\n",
    "                                     embedded), dim = 1))\n",
    "\n",
    "        return output, decoder_hidden.squeeze(0)\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module,\n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
    "\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> token\n",
    "        output = trg[0,:]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "# ENC_EMB_DIM = 256\n",
    "# DEC_EMB_DIM = 256\n",
    "# ENC_HID_DIM = 512\n",
    "# DEC_HID_DIM = 512\n",
    "# ATTN_DIM = 64\n",
    "# ENC_DROPOUT = 0.5\n",
    "# DEC_DROPOUT = 0.5\n",
    "\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = 64\n",
    "DEC_HID_DIM = 64\n",
    "ATTN_DIM = 8\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "\n",
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor(data_set.train_origin.seqs.T)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, src, trg):\n",
    "        self.src = Tensor(src)\n",
    "        self.trg = Tensor(trg)\n",
    "        \n",
    "    def __getitem__(self, n):\n",
    "        return self.src[n], self.trg[n]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextDataset(data_set.train_origin.seqs.T, data_set.train_trans.seqs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module,\n",
    "          loader: DataLoader,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, (src, trg) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-76691126a836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-4a356c60b297>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-32ade810506a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msrc_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtrg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ghq/github.com/Ryuta-Yamamoto/ML-study/.venv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-study",
   "language": "python",
   "name": "ml-study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
